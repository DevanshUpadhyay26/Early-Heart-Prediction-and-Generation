{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler  # for scaling data if needed\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#For Regression Algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#For Classification Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Data Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names\n",
    "list_of_column_names = list(df.columns)\n",
    "print('List of column names: ', list_of_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Features (Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns to keep\n",
    "columns_to_keep = [\n",
    "    'Age_shift_and_scale',\n",
    "    'Gender_shift_and_scale',\n",
    "    'ECG_shift_and_scale',\n",
    "    'CKMB_shift_and_scale',\n",
    "    'TROP-I_shift_and_scale',\n",
    "    'LAD_shift_and_scale',\n",
    "    'LCA_shift_and_scale',\n",
    "    'RCA_shift_and_scale',\n",
    "    'Systolic_shift_and_scale',\n",
    "    'Diastolic_shift_and_scale',\n",
    "    'Cholesterol_shift_and_scale',\n",
    "    'Chest_Pain_shift_and_scale',\n",
    "    'Diabetic_shift_and_scale',\n",
    "    'PHF /family history_shift_and_scale',\n",
    "    'MI',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only specified columns\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features and target\n",
    "X = df[['Age_shift_and_scale',\n",
    "        'Gender_shift_and_scale',\n",
    "        'ECG_shift_and_scale',\n",
    "        'CKMB_shift_and_scale',\n",
    "        'TROP-I_shift_and_scale',\n",
    "        'LAD_shift_and_scale',\n",
    "        'LCA_shift_and_scale',\n",
    "        'RCA_shift_and_scale',\n",
    "        'Systolic_shift_and_scale',\n",
    "        'Diastolic_shift_and_scale',\n",
    "        'Cholesterol_shift_and_scale',\n",
    "        'Chest_Pain_shift_and_scale',\n",
    "        'Diabetic_shift_and_scale',\n",
    "        'PHF /family history_shift_and_scale']]\n",
    "y = df['MI']  # Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_sampled = scaler.fit_transform(X_train)\n",
    "X_test_scaled_sampled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Train R-squared: 0.509282620440847\n",
      "Linear Regression - Test R-squared: 0.5378423200348161\n",
      "Decision Tree Regression - Train R-squared: 1.0\n",
      "Decision Tree Regression - Test R-squared: 0.772113640942978\n",
      "Random Forest Regression - Train R-squared: 0.9751874017042333\n",
      "Random Forest Regression - Test R-squared: 0.8490366814426757\n",
      "Gradient Boosting Regression - Train R-squared: 0.8384849705661939\n",
      "Gradient Boosting Regression - Test R-squared: 0.7772596415465602\n",
      "SVR Regression - Train R-squared: 0.8210519198486876\n",
      "SVR Regression - Test R-squared: 0.8553690588988845\n",
      "Neural Network Regression - Train R-squared: 0.8690382540210843\n",
      "Neural Network Regression - Test R-squared: 0.8540276125385691\n",
      "K-Nearest Neighbors Regression - Train R-squared: 0.860254692110315\n",
      "K-Nearest Neighbors Regression - Test R-squared: 0.8605802941769092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled_sampled, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = lin_reg.score(X_train_scaled_sampled, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = lin_reg.predict(X_test_scaled_sampled)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Linear Regression - Train R-squared:\", train_r2)\n",
    "print(\"Linear Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = dt_reg.score(X_train, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = dt_reg.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Decision Tree Regression - Train R-squared:\", train_r2)\n",
    "print(\"Decision Tree Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = rf_reg.score(X_train, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = rf_reg.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Random Forest Regression - Train R-squared:\", train_r2)\n",
    "print(\"Random Forest Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "gbm_reg = GradientBoostingRegressor()\n",
    "gbm_reg.fit(X_train, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = gbm_reg.score(X_train, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = gbm_reg.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Gradient Boosting Regression - Train R-squared:\", train_r2)\n",
    "print(\"Gradient Boosting Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svr_reg = SVR()\n",
    "svr_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = svr_reg.score(X_train_scaled, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = svr_reg.predict(X_test_scaled)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"SVR Regression - Train R-squared:\", train_r2)\n",
    "print(\"SVR Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp_reg = MLPRegressor()\n",
    "mlp_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = mlp_reg.score(X_train_scaled, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = mlp_reg.predict(X_test_scaled)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Neural Network Regression - Train R-squared:\", train_r2)\n",
    "print(\"Neural Network Regression - Test R-squared:\", test_r2)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Training R-squared\n",
    "train_r2 = knn_reg.score(X_train, y_train)\n",
    "\n",
    "# Testing R-squared\n",
    "y_pred_test = knn_reg.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"K-Nearest Neighbors Regression - Train R-squared:\", train_r2)\n",
    "print(\"K-Nearest Neighbors Regression - Test R-squared:\", test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                        | Train R-squared | Test R-squared |\n",
    "|------------------------------|-----------------|----------------|\n",
    "| Linear Regression            | 0.5093          | 0.5378         |\n",
    "| Decision Tree Regression     | 1.0000          | 0.7371         |\n",
    "| Random Forest Regression     | 0.9759          | 0.8450         |\n",
    "| Gradient Boosting Regression | 0.8385          | 0.7751         |\n",
    "| SVR Regression               | 0.8211          | 0.8554         |\n",
    "| Neural Network Regression    | 0.8659          | 0.8487         |\n",
    "| K-Nearest Neighbors Regression | 0.8603        | 0.8606         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_1 = \"D:/btech-final-year-project/Data Augmentation/Outputs_of_Feature-Models/Regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"linear_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Decision Tree Regression\n",
    "plt.scatter(y_test, y_pred, color='green')\n",
    "plt.title('Decision Tree Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"decision_tree_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Random Forest Regression\n",
    "plt.scatter(y_test, y_pred, color='red')\n",
    "plt.title('Random Forest Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"random_forest_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Gradient Boosting Regression\n",
    "plt.scatter(y_test, y_pred, color='orange')\n",
    "plt.title('Gradient Boosting Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"gradient_boosting_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Support Vector Regression (SVR)\n",
    "plt.scatter(y_test, y_pred, color='purple')\n",
    "plt.title('SVR Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"svr_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Neural Network Regression\n",
    "plt.scatter(y_test, y_pred, color='brown')\n",
    "plt.title('Neural Network Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"neural_network_regression_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "# K-Nearest Neighbors Regression\n",
    "plt.scatter(y_test, y_pred, color='black')\n",
    "plt.title('K-Nearest Neighbors Regression Model')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.savefig(os.path.join(output_dir_1, \"knn_regression_plot.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'logisticregression__C': 10.0}\n",
      "Train Accuracy for Logistic Regression: 0.9572895277207393\n",
      "Test Accuracy for Logistic Regression: 0.9540229885057471\n",
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93       182\n",
      "           1       0.98      1.00      0.99       266\n",
      "           2       0.91      0.92      0.92       161\n",
      "\n",
      "    accuracy                           0.95       609\n",
      "   macro avg       0.95      0.95      0.95       609\n",
      "weighted avg       0.95      0.95      0.95       609\n",
      "\n",
      "Train Accuracy for Decision Tree Classifier: 0.9634496919917864\n",
      "Test Accuracy for Decision Tree Classifier: 0.9573070607553367\n",
      "Classification Report for Decision Tree Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       182\n",
      "           1       0.99      0.98      0.99       266\n",
      "           2       0.90      0.96      0.92       161\n",
      "\n",
      "    accuracy                           0.96       609\n",
      "   macro avg       0.95      0.95      0.95       609\n",
      "weighted avg       0.96      0.96      0.96       609\n",
      "\n",
      "Best Parameters for Random Forest Classifier: {'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 10}\n",
      "Train Accuracy for Random Forest Classifier: 0.9843942505133471\n",
      "Test Accuracy for Random Forest Classifier: 0.9753694581280788\n",
      "Classification Report for Random Forest Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       182\n",
      "           1       0.98      1.00      0.99       266\n",
      "           2       0.96      0.96      0.96       161\n",
      "\n",
      "    accuracy                           0.98       609\n",
      "   macro avg       0.97      0.97      0.97       609\n",
      "weighted avg       0.98      0.98      0.98       609\n",
      "\n",
      "Train Accuracy for SVC: 0.9630390143737166\n",
      "Test Accuracy for SVC: 0.9704433497536946\n",
      "Classification Report for SVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       182\n",
      "           1       0.97      1.00      0.98       266\n",
      "           2       0.96      0.94      0.95       161\n",
      "\n",
      "    accuracy                           0.97       609\n",
      "   macro avg       0.97      0.96      0.97       609\n",
      "weighted avg       0.97      0.97      0.97       609\n",
      "\n",
      "Train Accuracy for K-NN Classifier: 0.9675564681724846\n",
      "Test Accuracy for K-NN Classifier: 0.9704433497536946\n",
      "Classification Report for K-NN Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       182\n",
      "           1       0.98      1.00      0.99       266\n",
      "           2       0.94      0.95      0.95       161\n",
      "\n",
      "    accuracy                           0.97       609\n",
      "   macro avg       0.97      0.97      0.97       609\n",
      "weighted avg       0.97      0.97      0.97       609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a pipeline with feature scaling and logistic regression\n",
    "pipe_logreg = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid_logreg = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1.0, 10.0]  # Regularization parameter\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_logreg = GridSearchCV(pipe_logreg, param_grid_logreg, cv=5, scoring='accuracy')\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate on train and test data\n",
    "best_logreg = grid_logreg.best_estimator_\n",
    "y_train_pred_logreg = best_logreg.predict(X_train)\n",
    "y_test_pred_logreg = best_logreg.predict(X_test)\n",
    "\n",
    "train_accuracy_logreg = accuracy_score(y_train, y_train_pred_logreg)\n",
    "test_accuracy_logreg = accuracy_score(y_test, y_test_pred_logreg)\n",
    "\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_logreg.best_params_)\n",
    "print(\"Train Accuracy for Logistic Regression:\", train_accuracy_logreg)\n",
    "print(\"Test Accuracy for Logistic Regression:\", test_accuracy_logreg)\n",
    "print(\"Classification Report for Logistic Regression:\\n\", classification_report(y_test, y_test_pred_logreg))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a decision tree classifier with max_depth parameter for pruning\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=5)  # Adjust max_depth as needed\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_dt = dt_classifier.predict(X_train)\n",
    "y_test_pred_dt = dt_classifier.predict(X_test)\n",
    "\n",
    "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "print(\"Train Accuracy for Decision Tree Classifier:\", train_accuracy_dt)\n",
    "print(\"Test Accuracy for Decision Tree Classifier:\", test_accuracy_dt)\n",
    "print(\"Classification Report for Decision Tree Classifier:\\n\", classification_report(y_test, y_test_pred_dt))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the parameter grid for random forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform randomized search with cross-validation\n",
    "rf_classifier = RandomForestClassifier()\n",
    "grid_rf = RandomizedSearchCV(rf_classifier, param_distributions=param_grid_rf, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate on train and test data\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_train_pred_rf = best_rf.predict(X_train)\n",
    "y_test_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"Best Parameters for Random Forest Classifier:\", grid_rf.best_params_)\n",
    "print(\"Train Accuracy for Random Forest Classifier:\", train_accuracy_rf)\n",
    "print(\"Test Accuracy for Random Forest Classifier:\", test_accuracy_rf)\n",
    "print(\"Classification Report for Random Forest Classifier:\\n\", classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train an SVM classifier with regularization parameter C\n",
    "svc_classifier = SVC(C=1.0)  # Adjust C value for regularization\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svc = svc_classifier.predict(X_train)\n",
    "y_test_pred_svc = svc_classifier.predict(X_test)\n",
    "\n",
    "train_accuracy_svc = accuracy_score(y_train, y_train_pred_svc)\n",
    "test_accuracy_svc = accuracy_score(y_test, y_test_pred_svc)\n",
    "\n",
    "print(\"Train Accuracy for SVC:\", train_accuracy_svc)\n",
    "print(\"Test Accuracy for SVC:\", test_accuracy_svc)\n",
    "print(\"Classification Report for SVC:\\n\", classification_report(y_test, y_test_pred_svc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a k-NN classifier with scaled data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred_knn = knn_classifier.predict(X_train_scaled)\n",
    "y_test_pred_knn = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "train_accuracy_knn = accuracy_score(y_train, y_train_pred_knn)\n",
    "test_accuracy_knn = accuracy_score(y_test, y_test_pred_knn)\n",
    "\n",
    "print(\"Train Accuracy for K-NN Classifier:\", train_accuracy_knn)\n",
    "print(\"Test Accuracy for K-NN Classifier:\", test_accuracy_knn)\n",
    "print(\"Classification Report for K-NN Classifier:\\n\", classification_report(y_test, y_test_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Train Accuracy | Test Accuracy |\n",
    "|---------------------------|----------------|---------------|\n",
    "| Logistic Regression       | 0.9573         | 0.9540        |\n",
    "| Decision Tree Classifier  | 0.9634         | 0.9573        |\n",
    "| Random Forest Classifier  | 0.9856         | 0.9721        |\n",
    "| Support Vector Classifier | 0.9630         | 0.9704        |\n",
    "| K-Nearest Neighbors       | 0.9676         | 0.9704        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_2 = \"D:/btech-final-year-project/Data Augmentation/Outputs_of_Feature-Models/Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(model_name, y_true, y_pred, output_dir):\n",
    "    \"\"\"\n",
    "    Generate and save a confusion matrix plot for the specified model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model for the plot title.\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        output_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Visualize confusion matrices for each classification model\n",
    "plot_confusion_matrix(\"Logistic Regression\", y_test, y_test_pred_logreg, output_dir_2)\n",
    "plot_confusion_matrix(\"Decision Tree Classifier\", y_test, y_test_pred_dt, output_dir_2)\n",
    "plot_confusion_matrix(\"Random Forest Classifier\", y_test, y_test_pred_rf, output_dir_2)\n",
    "plot_confusion_matrix(\"Support Vector Classifier\", y_test, y_test_pred_svc, output_dir_2)\n",
    "plot_confusion_matrix(\"K-Nearest Neighbors\", y_test, y_test_pred_knn, output_dir_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
